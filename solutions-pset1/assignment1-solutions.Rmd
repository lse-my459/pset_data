---
title: 'MY459: Assignment 1\n Solutions'
output: html_document
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "##"
)
```

### Overview

The purpose of this assignment is to familiarize yourself with R, RMarkdown, and `quanteda`.

As with the rest of assignments throughout the term, the goal is to write R code in the chunks within this RMarkdown file that will accomplish the tasks detailed below. Where questions are asked, include your answer in the after the question.

Start by loading the `quanteda` package.

```{r}
library("quanteda")
```

You will use the corpus of inaugural addresses by U.S. presidents `data_corpus_inaugural` for this part of the exercise. These data are available as soon as you load `quanteda` and are lazily-loaded once you access them.

Write code to inspect which inaugural address was the longest. Have a look at `texts()` to extract character vectors from the quanteda corpus object.

```{r}
which.max(nchar(texts(data_corpus_inaugural)))
```

### Question 1 (3 points)

Explain how this answer works.

How could you do this using a **quanteda** function?

```{r}
# possible answers include

ntoken(data_corpus_inaugural) %>%
  sort(decreasing = TRUE) %>%
  head(n = 5)

summary(data_corpus_inaugural)
```


### Question 2 (3 points)

Which president gave the longest speech and in what year was that?

```{r}
which.max(nchar(texts(data_corpus_inaugural)))
```

### Question 3 (4 points)


Generate a document-feature matrix from this corpus and call it `sotu_dfm`. Make sure that the text is put into __lowercase__ and words are __stemmed__ as preprocessing steps.

```{r}
sotu_dfm <- dfm(data_corpus_inaugural, tolower = TRUE, stem = TRUE)
```

Now, look at the 20 most common features in the document-feature matrix. Hint: `?topfeatures`

```{r}
# examine top 20 common features
topfeatures(sotu_dfm, n = 20)
```

What do most of those tokens have in common?

What step(s) might you take to focus on a more interesting set of word frequencies? Write code below to apply those steps, if you consider it necessary.

##### Model answer

*All of them are function words or punctuation. Words like that can be considered stopwords and are often excluded from bag-of-words analyses.*

Now generate a second document-feature matrix but remove punctuation and remove stopwords. Call this object `sotu_dfm_sp`.

```{r}
sotu_dfm_sp <- dfm(data_corpus_inaugural, tolower = TRUE, stem = TRUE, remove = stopwords("english"), remove_punct = TRUE)
```

Now look at the 20 most common features again. The most common words are now no longer uninteresting words.

```{r}
topfeatures(sotu_dfm_sp, n = 20)
```

To go further, we could also have used the `remove_numbers = TRUE` argument. Note that in **quanteda**, it is possible to tokenize the corpus first, using `tokens()`, and then create the dfm from those tokens. This provides an additional amount of control, through the many functions available in the [`tokens_*()` functions](http://docs.quanteda.io/reference/#section-tokens-functions).

### Question 4 (3 points)

Write code to figure out how many different types of punctuation and how many different types of stopwords were removed.

```{r}
# Simple
dfm(data_corpus_inaugural, tolower = TRUE, stem = TRUE, remove = stopwords("english"), remove_punct = TRUE, verbose = TRUE)

# More complicated, hand-crafted solution
nfeat(dfm(data_corpus_inaugural, tolower = TRUE, stem = TRUE)) -
  nfeat(dfm(data_corpus_inaugural, tolower = TRUE, stem = TRUE, remove = stopwords("english"), remove_punct = TRUE))
```

### Question 5 (3 points)

Plot a wordcloud of the dfm that has the stopwords and punctuation characters, and numbers removed, but without stemming. Plot this only for words with a minimum count of 10. Hint: Use `textplot_wordcloud()` to generate the plot.

```{r, warning=FALSE}
# Generate dfm
sotu_dfm_spn <- dfm(data_corpus_inaugural, tolower = TRUE, stem = FALSE, remove = stopwords("english"), remove_punct = TRUE, remove_numbers = TRUE)
# Remove words (types) with less than 10 occurrences
sotu_dfm_spn <- dfm_trim(sotu_dfm_spn, min_termfreq = 10, verbose = TRUE)
set.seed(123)
textplot_wordcloud(sotu_dfm_spn, min_size=.2, max_size=2, max_words=50)
```

### Question 6 (2 points)

Search for the word "Christmas" from the `data_corpus_irishbudget2010` object. What is the context in which this term is being used?

```{r}
# install.packages("quanteda.textmodels")
data("data_corpus_irishbudget2010", package = "quanteda.textmodels")
kwic(data_corpus_irishbudget2010, "Christmas*")
```

Search for the word "Irish" and save the results of this keywords-in-context object to a new object called `irish_kwic`. Plot an "x-ray" plot for this kwic, in the code bloc below:

```{r fig.width = 10}
irish_kwic <- kwic(data_corpus_irishbudget2010, pattern = "Irish")
textplot_xray(irish_kwic)
```

#### Question 7 (2 points)

Examine the context of words *related to* "disaster". Hint: you can use the stem of the word along with setting the `regex` argument to `TRUE`. Execute a query using a pattern match that returns different variations of words based on "disaster" (such as disasters, disastrous, disastrously, etc.).

```{r}
# Other regular expression are possible here
kwic(data_corpus_irishbudget2010, pattern = "disast[A-Za-z]*", valuetype = "regex")
```

